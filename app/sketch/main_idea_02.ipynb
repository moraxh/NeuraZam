{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1123a523",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51be18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from torch_audiomentations import AddBackgroundNoise, AddColoredNoise, Gain, PitchShift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80a40a",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a20299",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./cache\"\n",
    "SONGS_DIR = f\"{CACHE_DIR}/songs\"\n",
    "SEGMENTS_DIR = f\"{CACHE_DIR}/segments\"  \n",
    "SPECTOGRAMS_DIR = f\"{CACHE_DIR}/spectograms\"\n",
    "\n",
    "ASSETS_DIR = \"./assets\"\n",
    "BACKGROUND_NOISE_DIR = f\"{ASSETS_DIR}/background_noises\"\n",
    "\n",
    "DATASET_FILE = f\"{CACHE_DIR}/dataset.csv\"\n",
    "EMBEDDINGS_FILE = f\"{CACHE_DIR}/embeddings.npz\" \n",
    "TRAINED_MODEL_FILE = f\"{CACHE_DIR}/trained_model.pt\"\n",
    "\n",
    "# SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/1Y0Qk1K1DEMXeKgvjjnN7m?si=80a2a297dded480b\" # 10 songs\n",
    "# SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/34NbomaTu7YuOYnky8nLXL?si=4bf54104cf4c480c\" # 50 songs\n",
    "SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/0sDahzOkMWOmLXfTMf2N4N?si=94c4a9796f934a34\" # 100 songs\n",
    "# SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/0MK9YSzaQn2D0fsuIHa94B?si=c2fb987f3b94496e\" # 306 songs\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 for faster training\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "SEGMENT_DURATION = 3 # seconds\n",
    "\n",
    "# CNN model parameters  \n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 0\n",
    "SHUFFLE = True\n",
    "\n",
    "NUM_TESTS = 100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create needed directories\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(SONGS_DIR, exist_ok=True)\n",
    "os.makedirs(SPECTOGRAMS_DIR, exist_ok=True)\n",
    "os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "os.makedirs(ASSETS_DIR, exist_ok=True)  \n",
    "os.makedirs(BACKGROUND_NOISE_DIR, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25660dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(os.listdir(BACKGROUND_NOISE_DIR)) == 0):\n",
    "  raise Exception(f\"Please download background noise files to {BACKGROUND_NOISE_DIR} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816cbdb",
   "metadata": {},
   "source": [
    "# Download Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faab02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The songs directory is not empty, skipping download.\n",
      "The songs directory already has the names changed, skipping renaming.\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(SONGS_DIR)) == 0:\n",
    "  print(f\"Songs directory is empty, downloading songs from {SPOTIFY_PLAYLIST_URL}\")\n",
    "\n",
    "  # Download songs\n",
    "  subprocess.run(\n",
    "    [\"spotdl\", SPOTIFY_PLAYLIST_URL, \"--bitrate\", \"96k\"],\n",
    "    check=True,\n",
    "    cwd=SONGS_DIR,\n",
    "  )\n",
    "else:\n",
    "  print(f\"The songs directory is not empty, skipping download.\")\n",
    "\n",
    "def is_int(s):\n",
    "  try:\n",
    "    int(s)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "# Check if the songs dir already has the names changed\n",
    "if not (all(is_int(song.split(\".\")[0]) for song in os.listdir(SONGS_DIR))): \n",
    "  print(f\"Renaming songs in {SONGS_DIR} to integers\")\n",
    "  for i, song in enumerate(tqdm(os.listdir(SONGS_DIR), desc=\"Renaming songs\", unit=\"song\")):\n",
    "    song_path = os.path.join(SONGS_DIR, song)\n",
    "    song_extension = song.split(\".\")[-1]\n",
    "\n",
    "    file_path = os.path.join(SONGS_DIR, f\"{i}.{song_extension}\")\n",
    "\n",
    "    os.rename(\n",
    "      song_path,\n",
    "      file_path,\n",
    "    )\n",
    "else: \n",
    "  print(f\"The songs directory already has the names changed, skipping renaming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36e1cb",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76921026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founded spectograms, skiping it\n"
     ]
    }
   ],
   "source": [
    "mel_spec_transform = torch.nn.Sequential(\n",
    "    MelSpectrogram(n_fft=2048, hop_length=512, n_mels=128, f_min=20, f_max=8000),\n",
    "    AmplitudeToDB(),\n",
    "  ).to(device)\n",
    "\n",
    "def get_spectogram(waveform, as_numpy=True):\n",
    "  with torch.no_grad(): \n",
    "    mel_spec = mel_spec_transform(waveform)\n",
    "  mel_spec = mel_spec.to(device)  # Move to GPU if available  \n",
    "\n",
    "  if as_numpy:\n",
    "    return mel_spec.cpu().detach().numpy()\n",
    "\n",
    "  return mel_spec \n",
    "\n",
    "def get_waveform_n_sr_from_file(file_path):\n",
    "  waveform, sr = torchaudio.load(file_path, normalize=True)\n",
    "\n",
    "  if sr != TARGET_SAMPLE_RATE:  \n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SAMPLE_RATE)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "  waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "  waveform = waveform.unsqueeze(0)  \n",
    "  waveform = waveform.to(device)  # Move to GPU if available\n",
    "\n",
    "  return waveform, TARGET_SAMPLE_RATE\n",
    "\n",
    "def get_global_mean_std(files):\n",
    "  sum_        = 0.0\n",
    "  sum_sq      = 0.0\n",
    "  count       = 0\n",
    "\n",
    "  for file in files:\n",
    "    file_path = os.path.join(SONGS_DIR, file) \n",
    "    waveform, sr = get_waveform_n_sr_from_file(file_path) \n",
    "    spec_np = get_spectogram(waveform, as_numpy=False)\n",
    "\n",
    "    vals = spec_np.flatten()\n",
    "\n",
    "    sum_ += vals.sum()\n",
    "    sum_sq += (vals ** 2).sum()\n",
    "    count += vals.numel()\n",
    "\n",
    "  mean_global = sum_ / count\n",
    "  var_global  = sum_sq / count - mean_global**2\n",
    "  std_global  = torch.sqrt(var_global)\n",
    "\n",
    "  return mean_global.cpu().detach().numpy(), std_global.cpu().detach().numpy()\n",
    "\n",
    "def save_spectogram(segment, song_id, chunk_id, aug_name, variation=0):\n",
    "    file_name = f\"{song_id}_{chunk_id}_{aug_name}_{variation}.npz\"\n",
    "\n",
    "    # Store as compressed file\n",
    "    np.savez_compressed(\n",
    "      os.path.join(SPECTOGRAMS_DIR, file_name),\n",
    "      data=segment\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      \"song_id\": song_id,\n",
    "      \"file_name\": file_name,\n",
    "      \"chunk_id\": chunk_id,\n",
    "      \"aug_name\": aug_name,\n",
    "      \"variation\": variation,\n",
    "    }\n",
    "\n",
    "global_mean, global_std = get_global_mean_std(os.listdir(SONGS_DIR))\n",
    "\n",
    "def extract_features():\n",
    "  records = []\n",
    "\n",
    "  for song in tqdm(os.listdir(SONGS_DIR), desc=\"Extracting features of the songs\", unit=\"song\"):\n",
    "    song_path = os.path.join(SONGS_DIR, song)\n",
    "    song_id = os.path.splitext(song)[0]\n",
    "\n",
    "    waveform, sr = get_waveform_n_sr_from_file(song_path)\n",
    "\n",
    "    segments_samples = int(SEGMENT_DURATION * sr)\n",
    "    total_samples = waveform.shape[2]\n",
    "\n",
    "    hop_length = segments_samples // 2\n",
    "    chunk_id = 0\n",
    "    for start in range(0, total_samples - segments_samples + 1, hop_length):\n",
    "\n",
    "      if start + segments_samples > total_samples:\n",
    "        break\n",
    "\n",
    "      end = start + segments_samples\n",
    "      segment = waveform[:, :, start:end]\n",
    "\n",
    "      with torch.no_grad():\n",
    "        mel_spec = get_spectogram(segment)\n",
    "\n",
    "      # Normalize using the global mean & st\n",
    "      mel_spec = (mel_spec - global_mean) / global_std\n",
    "\n",
    "      segment_file = f\"{song_id}_{chunk_id}.wav\"\n",
    "\n",
    "      # Save the segment\n",
    "      torchaudio.save(\n",
    "        os.path.join(SEGMENTS_DIR, segment_file),\n",
    "        segment.cpu().squeeze(0),\n",
    "        sr,\n",
    "      )\n",
    "\n",
    "      records.append(\n",
    "        save_spectogram(mel_spec, song_id, chunk_id, \"original\")\n",
    "      )\n",
    "\n",
    "      chunk_id += 1\n",
    "    \n",
    "  # Save the dataframe\n",
    "  df = pd.DataFrame(records)\n",
    "  df.to_csv(DATASET_FILE, index=False)\n",
    "\n",
    "if (len(os.listdir(SPECTOGRAMS_DIR)) == 0 or len(os.listdir(SEGMENTS_DIR)) == 0):\n",
    "  print(\"Not founded spectograms, extracting them...\")\n",
    "  extract_features()\n",
    "else:\n",
    "  print(\"Founded spectograms, skiping it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd9c4d",
   "metadata": {},
   "source": [
    "# Balance classes using augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b2ba01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset already has augmented data, skipping augmentation.\n"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "    \"background_noise\": AddBackgroundNoise(\n",
    "      p=0.5,\n",
    "      background_paths=BACKGROUND_NOISE_DIR,\n",
    "      min_snr_in_db=10,\n",
    "      max_snr_in_db=20,\n",
    "      output_type=\"dict\"\n",
    "    ),  \n",
    "    \"colored_noise\": AddColoredNoise(\n",
    "      p=0.5,\n",
    "      min_snr_in_db=10,\n",
    "      max_snr_in_db=20,\n",
    "      output_type=\"dict\"\n",
    "    ),\n",
    "    \"gain\": Gain(\n",
    "      p=0.5,  \n",
    "      min_gain_in_db=-10,\n",
    "      max_gain_in_db=20,\n",
    "      output_type=\"dict\"\n",
    "    ),\n",
    "    \"pitch_shift\": PitchShift(\n",
    "      p=0.5,\n",
    "      min_transpose_semitones=-2,\n",
    "      max_transpose_semitones=2,\n",
    "      sample_rate=TARGET_SAMPLE_RATE,\n",
    "      output_type=\"dict\"\n",
    "    ),\n",
    "  }\n",
    "\n",
    "def augment_data():\n",
    "  df = pd.read_csv(DATASET_FILE)\n",
    "\n",
    "  if (df[\"aug_name\"] == \"original\").all():\n",
    "    print(\"The dataset only has original data, applying augmentation.\")\n",
    "\n",
    "    # Get the number of clases per song_id\n",
    "    print(df[\"song_id\"].value_counts())\n",
    "\n",
    "    number_of_samples_target = df[\"song_id\"].value_counts().max() * 1.2\n",
    "\n",
    "    print(f\"Target number of samples: {number_of_samples_target}\")\n",
    "\n",
    "    unique_song_ids = df[\"song_id\"].unique()\n",
    "    \n",
    "    for song_id in tqdm(unique_song_ids, desc=\"Augmenting data\", unit=\"song\"):\n",
    "      song_df = df[df[\"song_id\"] == song_id]\n",
    "\n",
    "      while df[df[\"song_id\"] == song_id].shape[0] < number_of_samples_target:\n",
    "        # Get a random sample of the original data\n",
    "        sample = song_df.sample(n=1, random_state=np.random.randint(0, 10000)).iloc[0]\n",
    "        segment_file = f\"{song_id}_{sample['chunk_id']}.wav\"\n",
    "        file_path = os.path.join(SEGMENTS_DIR, segment_file)\n",
    "\n",
    "        # Apply a random augmentation\n",
    "        aug_name = np.random.choice(list(augmentations.keys()))\n",
    "        augment = augmentations[aug_name]\n",
    "\n",
    "        waveform, sr = get_waveform_n_sr_from_file(file_path)\n",
    "        mel_spec = get_spectogram(waveform)\n",
    "        augmented_segment = augment(waveform, sample_rate=TARGET_SAMPLE_RATE)['samples']\n",
    "        mel_spec = get_spectogram(augmented_segment)\n",
    "        mel_spec = (mel_spec - global_mean) / global_std  \n",
    "\n",
    "        # Check if its a variation\n",
    "        duplicated = df[\n",
    "          (df[\"song_id\"] == song_id) & \n",
    "          (df[\"chunk_id\"] == sample[\"chunk_id\"]) &\n",
    "          (df[\"aug_name\"] == aug_name)\n",
    "        ]\n",
    "\n",
    "        if duplicated.shape[0] > 0:\n",
    "          variation_num = duplicated[\"variation\"].max() + 1\n",
    "        else:\n",
    "          variation_num = 0\n",
    "        \n",
    "\n",
    "        record = save_spectogram(\n",
    "          mel_spec,\n",
    "          song_id,\n",
    "          sample[\"chunk_id\"],\n",
    "          aug_name,\n",
    "          variation=variation_num \n",
    "        )\n",
    "\n",
    "        # Append the record to the dataframe\n",
    "        df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)\n",
    "\n",
    "    df.to_csv(DATASET_FILE, index=False)\n",
    "    print(df[\"song_id\"].value_counts())\n",
    "\n",
    "  else:\n",
    "    print(\"The dataset already has augmented data, skipping augmentation.\")\n",
    "    return\n",
    "  \n",
    "augment_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9b536",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0908cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectogramDataset(Dataset):\n",
    "  def __init__(self, spectograms_dir=SPECTOGRAMS_DIR, transform=None):\n",
    "    self.spectograms_files = os.listdir(SPECTOGRAMS_DIR)\n",
    "    self.spectograms_dir = spectograms_dir\n",
    "    self.transform = transform\n",
    "    dataset = pd.read_csv(DATASET_FILE)\n",
    "    self.labels = dataset[\"song_id\"]\n",
    "    self.classes = dataset['song_id'].unique()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.spectograms_files)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    file_name = self.spectograms_files[idx]\n",
    "    file_path = os.path.join(self.spectograms_dir, file_name)\n",
    "\n",
    "    mel_spec = np.load(file_path)[\"data\"] \n",
    "\n",
    "    if mel_spec.ndim == 4:\n",
    "      mel_spec = mel_spec.squeeze(0).squeeze(0)\n",
    "    elif mel_spec.ndim == 3:\n",
    "      mel_spec = mel_spec.squeeze(0) \n",
    "        \n",
    "    mel_spec = mel_spec[np.newaxis, ...]\n",
    "\n",
    "    label = int(file_name.split(\"_\")[0])  # Get the song id from the file name \n",
    "\n",
    "    if self.transform:\n",
    "      mel_spec = self.transform(mel_spec) \n",
    "\n",
    "    mel_spec = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "    label = torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    return mel_spec, label\n",
    "\n",
    "def get_spectograms():\n",
    "  dataset = SpectogramDataset()\n",
    "\n",
    "  idx = list(range(len(dataset)))\n",
    "  train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=dataset.labels)\n",
    "\n",
    "  train_subset = Subset(dataset, train_idx)   \n",
    "  test_subset  = Subset(dataset, test_idx)\n",
    "\n",
    "  return train_subset, test_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff25f6",
   "metadata": {},
   "source": [
    "# Using a CNN te extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9191948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of classes: 99\n",
      "Model not trained. Training model with cuda...\n",
      "Epoch 0: Train Loss 3.0305, Val Loss 2.1618, Train Acc 0.2618, Val Acc 0.4368, ETA: 17912.929260730743\n",
      "Epoch 1: Train Loss 1.1905, Val Loss 0.8628, Train Acc 0.6820, Val Acc 0.7570, ETA: 9689.028897047043\n",
      "Epoch 2: Train Loss 0.6246, Val Loss 0.4328, Train Acc 0.8309, Val Acc 0.8829, ETA: 6902.692459106445\n",
      "Epoch 3: Train Loss 0.3788, Val Loss 0.2548, Train Acc 0.8983, Val Acc 0.9370, ETA: 5488.51508128643\n",
      "Epoch 4: Train Loss 0.2661, Val Loss 0.1896, Train Acc 0.9294, Val Acc 0.9495, ETA: 4633.679958629608\n",
      "Epoch 5: Train Loss 0.2079, Val Loss 0.1394, Train Acc 0.9467, Val Acc 0.9680, ETA: 4045.557001233101\n",
      "Epoch 6: Train Loss 0.1765, Val Loss 0.1269, Train Acc 0.9560, Val Acc 0.9728, ETA: 3616.589928490775\n",
      "Epoch 7: Train Loss 0.1560, Val Loss 0.1899, Train Acc 0.9605, Val Acc 0.9513, ETA: 3288.9591942429543\n",
      "Epoch 8: Train Loss 0.1325, Val Loss 0.0921, Train Acc 0.9669, Val Acc 0.9818, ETA: 3025.115770975749\n",
      "Epoch 9: Train Loss 0.1238, Val Loss 0.0999, Train Acc 0.9692, Val Acc 0.9746, ETA: 2805.3142102718352\n",
      "Epoch 10: Train Loss 0.1167, Val Loss 0.0928, Train Acc 0.9721, Val Acc 0.9766, ETA: 2620.074185458097\n",
      "Epoch 11: Train Loss 0.1121, Val Loss 0.0723, Train Acc 0.9739, Val Acc 0.9874, ETA: 2459.3837766051292\n",
      "Epoch 12: Train Loss 0.1121, Val Loss 0.1777, Train Acc 0.9711, Val Acc 0.9485, ETA: 2317.9986472863416\n",
      "Epoch 13: Train Loss 0.0934, Val Loss 0.1313, Train Acc 0.9782, Val Acc 0.9659, ETA: 2191.638246229717\n",
      "Epoch 14: Train Loss 0.0927, Val Loss 0.1181, Train Acc 0.9782, Val Acc 0.9697, ETA: 2076.1889081954955\n",
      "Epoch 15: Train Loss 0.0971, Val Loss 0.1203, Train Acc 0.9769, Val Acc 0.9684, ETA: 1970.6477534025908\n",
      "Epoch 16: Train Loss 0.0977, Val Loss 0.1616, Train Acc 0.9761, Val Acc 0.9584, ETA: 1873.7099270820618\n",
      "Epoch 17: Train Loss 0.0899, Val Loss 0.0907, Train Acc 0.9784, Val Acc 0.9769, ETA: 1783.5570493936539\n",
      "Early stopping at epoch 18/50\n",
      "Best model loaded from epoch 18/50\n",
      "Model saved to ./cache/trained_model.pt\n",
      "Model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, num_classes=256):\n",
    "    super(CNN, self).__init__()\n",
    "\n",
    "    self.conv_block = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout2d(0.1),\n",
    "      nn.MaxPool2d((2, 1)),\n",
    "      \n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d((2, 2)),\n",
    "\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d((2, 2)),\n",
    "\n",
    "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(256),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.AdaptiveAvgPool2d((4, 4)),\n",
    "    )\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(256 * 4 * 4, 1024),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.3),\n",
    "      nn.Linear(1024, num_classes),\n",
    "    )\n",
    "\n",
    "    self.scaler = GradScaler(device)\n",
    "\n",
    "    self.training_ETA = 0.0\n",
    "    self.total_epochs = 0\n",
    "    self.current_epoch = 0\n",
    "    self.is_model_trained = False\n",
    "  \n",
    "  def forward(self, x):\n",
    "    if x.dim() > 4:\n",
    "      x = x.squeeze(1)\n",
    "\n",
    "    x = self.conv_block(x)\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def fit(self, train_loader, test_loader, epochs=50, learning_rate=0.001, patience=7):\n",
    "    self.total_epochs = epochs\n",
    "    training_epoch_duration = []\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      self.train()\n",
    "      epoch_loss = 0.0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      start_time = time.time()\n",
    "      self.current_epoch = epoch\n",
    "\n",
    "      i = 0\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=device):\n",
    "          outputs = self(inputs)\n",
    "          loss = loss_function(outputs, labels)\n",
    "        \n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "      \n",
    "      # Calculate the average loss for the epoch\n",
    "      train_acc = correct / total\n",
    "      avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "      # Validation\n",
    "      val_loss = 0.0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      self.eval()\n",
    "      with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "          inputs = inputs.to(device, non_blocking=True)\n",
    "          labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "          with autocast(device_type=device):\n",
    "            outputs = self(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "          val_loss += loss.item()\n",
    "          _, predicted = outputs.max(1)\n",
    "          total += labels.size(0)\n",
    "          correct += predicted.eq(labels).sum().item()\n",
    "      \n",
    "      val_acc = correct / total\n",
    "      avg_val_loss = val_loss / len(test_loader)\n",
    "      scheduler.step(avg_val_loss)\n",
    "\n",
    "      # Early stopping\n",
    "      if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = self.state_dict()    \n",
    "        patience_counter = 0\n",
    "      else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "          print(f\"Early stopping at epoch {epoch}/{epochs}\")\n",
    "          break\n",
    "\n",
    "      # Calculate the training ETA\n",
    "      elapsed_time = time.time() - start_time\n",
    "      training_epoch_duration.append(elapsed_time)\n",
    "      self.training_ETA = (sum(training_epoch_duration) / (self.current_epoch + 1)) * (self.total_epochs - self.current_epoch)\n",
    "\n",
    "      print(f\"Epoch {epoch}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}, ETA: {self.training_ETA}\")\n",
    "\n",
    "    if best_model_state:\n",
    "      self.load_state_dict(best_model_state)\n",
    "      print(f\"Best model loaded from epoch {self.current_epoch}/{self.total_epochs}\")\n",
    "    self.is_model_trained = True\n",
    "\n",
    "  def predict(self, X):\n",
    "    self.eval()\n",
    "\n",
    "    X = torch.as_tensor(X, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(X, list):\n",
    "        X = torch.stack(X)\n",
    "    elif X.ndim == 2:\n",
    "        X = X.unsqueeze(0)\n",
    "\n",
    "    X = X.to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=device):\n",
    "        embeddings = self(X)\n",
    "\n",
    "    embeddings = embeddings.cpu()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "  def save_trained_model(self):\n",
    "    # Save the model\n",
    "    torch.save(self.state_dict(), TRAINED_MODEL_FILE)\n",
    "\n",
    "    print(f\"Model saved to {TRAINED_MODEL_FILE}\")\n",
    "  \n",
    "  def load_trained_model(self):\n",
    "    # Load the model\n",
    "    map_location = torch.device(device)\n",
    "    self.load_state_dict(torch.load(TRAINED_MODEL_FILE, map_location=map_location))\n",
    "    self.eval()\n",
    "    self.is_model_trained = True\n",
    "\n",
    "def initialize_model():\n",
    "  train_ds, test_ds = get_spectograms()\n",
    "\n",
    "  # Create DataLoaders for train and test sets\n",
    "  train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=SHUFFLE, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "  )\n",
    "\n",
    "  test_loader = DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "  )\n",
    "\n",
    "  n_classes = len(list(set(train_loader.dataset.dataset.classes) | set(test_loader.dataset.dataset.classes)))\n",
    "  print(f\"Num of classes: {n_classes}\")\n",
    "\n",
    "  model = CNN(n_classes).to(device)\n",
    "\n",
    "  if torch.__version__ >= '2.0':\n",
    "    model = torch.compile(model)\n",
    "\n",
    "  train_model(model, train_loader, test_loader)\n",
    "\n",
    "  return model \n",
    "\n",
    "def train_model(model, train_loader, test_loader):\n",
    "  # Check if the model is already trained\n",
    "  if (os.path.exists(TRAINED_MODEL_FILE)):\n",
    "    print(f\"Model already trained. Loading model...\")\n",
    "    model.load_trained_model()\n",
    "  else:\n",
    "    print(f\"Model not trained. Training model with {device}...\")\n",
    "    model.fit(train_loader=train_loader, test_loader=test_loader, epochs=50, learning_rate=0.001)\n",
    "    model.save_trained_model()\n",
    "    print(f\"Model trained and saved.\")\n",
    "\n",
    "model = initialize_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
