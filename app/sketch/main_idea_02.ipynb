{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1123a523",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e51be18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_metric_learning import miners, losses, distances\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from torch_audiomentations import AddBackgroundNoise, AddColoredNoise, Gain, PitchShift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80a40a",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a20299",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"./cache\"\n",
    "SONGS_DIR = f\"{CACHE_DIR}/songs\"\n",
    "SEGMENTS_DIR = f\"{CACHE_DIR}/segments\"  \n",
    "SPECTOGRAMS_DIR = f\"{CACHE_DIR}/spectograms\"\n",
    "\n",
    "ASSETS_DIR = \"./assets\"\n",
    "BACKGROUND_NOISE_DIR = f\"{ASSETS_DIR}/background_noises\"\n",
    "\n",
    "DATASET_FILE = f\"{CACHE_DIR}/dataset.csv\"\n",
    "EMBEDDINGS_FILE = f\"{CACHE_DIR}/embeddings.npz\" \n",
    "TRAINED_MODEL_FILE = f\"{CACHE_DIR}/trained_model.pt\"\n",
    "\n",
    "# SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/1Y0Qk1K1DEMXeKgvjjnN7m?si=80a2a297dded480b\" # 10 songs\n",
    "# SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/34NbomaTu7YuOYnky8nLXL?si=4bf54104cf4c480c\" # 50 songs\n",
    "SPOTIFY_PLAYLIST_URL=\"https://open.spotify.com/playlist/0MK9YSzaQn2D0fsuIHa94B?si=c2fb987f3b94496e\" # 306 songs\n",
    "\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "SEGMENT_DURATION = 3 # seconds\n",
    "\n",
    "# CNN model parameters  \n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "SHUFFLE = True\n",
    "\n",
    "NUM_TESTS = 100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create needed directories\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(SONGS_DIR, exist_ok=True)\n",
    "os.makedirs(SPECTOGRAMS_DIR, exist_ok=True)\n",
    "os.makedirs(SEGMENTS_DIR, exist_ok=True)\n",
    "os.makedirs(ASSETS_DIR, exist_ok=True)  \n",
    "os.makedirs(BACKGROUND_NOISE_DIR, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25660dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(os.listdir(BACKGROUND_NOISE_DIR)) == 0):\n",
    "  raise Exception(f\"Please download background noise files to {BACKGROUND_NOISE_DIR} directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816cbdb",
   "metadata": {},
   "source": [
    "# Download Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faab02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The songs directory is not empty, skipping download.\n",
      "Renaming songs in ./cache/songs to integers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renaming songs: 100%|██████████| 10/10 [00:00<00:00, 4970.14song/s]\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(SONGS_DIR)) == 0:\n",
    "  print(f\"Songs directory is empty, downloading songs from {SPOTIFY_PLAYLIST_URL}\")\n",
    "\n",
    "  # Download songs\n",
    "  subprocess.run(\n",
    "    [\"spotdl\", SPOTIFY_PLAYLIST_URL, \"--bitrate\", \"96k\"],\n",
    "    check=True,\n",
    "    cwd=SONGS_DIR,\n",
    "  )\n",
    "else:\n",
    "  print(f\"The songs directory is not empty, skipping download.\")\n",
    "\n",
    "# Check if the songs dir already has the names changed\n",
    "if not (all(isinstance(song.split(\".\")[0], int) for song in os.listdir(SONGS_DIR))): \n",
    "  print(f\"Renaming songs in {SONGS_DIR} to integers\")\n",
    "  for i, song in enumerate(tqdm(os.listdir(SONGS_DIR), desc=\"Renaming songs\", unit=\"song\")):\n",
    "    song_path = os.path.join(SONGS_DIR, song)\n",
    "    song_extension = song.split(\".\")[-1]\n",
    "\n",
    "    file_path = os.path.join(SONGS_DIR, f\"{i}.{song_extension}\")\n",
    "\n",
    "    os.rename(\n",
    "      song_path,\n",
    "      file_path,\n",
    "    )\n",
    "else: \n",
    "  print(f\"The songs directory already has the names changed, skipping renaming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36e1cb",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76921026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founded spectograms, skiping it\n"
     ]
    }
   ],
   "source": [
    "mel_spec_transform = torch.nn.Sequential(\n",
    "    MelSpectrogram(n_fft=2048, hop_length=512, n_mels=128, f_min=20, f_max=8000),\n",
    "    AmplitudeToDB(),\n",
    "  )\n",
    "# Compile the model\n",
    "mel_spec_transform = torch.jit.script(mel_spec_transform.to(device))\n",
    "\n",
    "def get_spectogram(waveform, as_numpy=True):\n",
    "  with torch.no_grad(): \n",
    "    mel_spec = mel_spec_transform(waveform)\n",
    "  mel_spec = mel_spec.to(device)  # Move to GPU if available  \n",
    "\n",
    "  if as_numpy:\n",
    "    return mel_spec.cpu().detach().numpy()\n",
    "\n",
    "  return mel_spec \n",
    "\n",
    "def get_waveform_n_sr_from_file(file_path):\n",
    "  waveform, sr = torchaudio.load(file_path, normalize=True)\n",
    "\n",
    "  if sr != TARGET_SAMPLE_RATE:  \n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SAMPLE_RATE)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "  waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono\n",
    "  waveform = waveform.unsqueeze(0)  \n",
    "  waveform = waveform.to(device)  # Move to GPU if available\n",
    "\n",
    "  return waveform, TARGET_SAMPLE_RATE\n",
    "\n",
    "def get_global_mean_std(files):\n",
    "  sum_        = 0.0\n",
    "  sum_sq      = 0.0\n",
    "  count       = 0\n",
    "\n",
    "  for file in files:\n",
    "    file_path = os.path.join(SONGS_DIR, file) \n",
    "    waveform, sr = get_waveform_n_sr_from_file(file_path) \n",
    "    spec_np = get_spectogram(waveform, as_numpy=False)\n",
    "\n",
    "    vals = spec_np.flatten()\n",
    "\n",
    "    sum_ += vals.sum()\n",
    "    sum_sq += (vals ** 2).sum()\n",
    "    count += vals.numel()\n",
    "\n",
    "  mean_global = sum_ / count\n",
    "  var_global  = sum_sq / count - mean_global**2\n",
    "  std_global  = torch.sqrt(var_global)\n",
    "\n",
    "  return mean_global.cpu().detach().numpy(), std_global.cpu().detach().numpy()\n",
    "\n",
    "def save_spectogram(segment, song_id, chunk_id, aug_name, variation=0):\n",
    "    file_name = f\"{song_id}_{chunk_id}_{aug_name}_{variation}.npz\"\n",
    "\n",
    "    # Store as compressed file\n",
    "    np.savez_compressed(\n",
    "      os.path.join(SPECTOGRAMS_DIR, file_name),\n",
    "      data=segment\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      \"song_id\": song_id,\n",
    "      \"file_name\": file_name,\n",
    "      \"chunk_id\": chunk_id,\n",
    "      \"aug_name\": aug_name,\n",
    "      \"variation\": variation,\n",
    "    }\n",
    "\n",
    "global_mean, global_std = get_global_mean_std(os.listdir(SONGS_DIR))\n",
    "\n",
    "def extract_features():\n",
    "  records = []\n",
    "\n",
    "  for song in tqdm(os.listdir(SONGS_DIR), desc=\"Extracting features of the songs\", unit=\"song\"):\n",
    "    song_path = os.path.join(SONGS_DIR, song)\n",
    "    song_id = os.path.splitext(song)[0]\n",
    "\n",
    "    waveform, sr = get_waveform_n_sr_from_file(song_path)\n",
    "\n",
    "    segments_samples = int(SEGMENT_DURATION * sr)\n",
    "    total_samples = waveform.shape[2]\n",
    "\n",
    "    hop_length = segments_samples // 2\n",
    "    chunk_id = 0\n",
    "    for start in range(0, total_samples - segments_samples + 1, hop_length):\n",
    "\n",
    "      if start + segments_samples > total_samples:\n",
    "        break\n",
    "\n",
    "      end = start + segments_samples\n",
    "      segment = waveform[:, :, start:end]\n",
    "\n",
    "      with torch.no_grad():\n",
    "        mel_spec = get_spectogram(segment)\n",
    "\n",
    "      # Normalize using the global mean & st\n",
    "      mel_spec = (mel_spec - global_mean) / global_std\n",
    "\n",
    "      segment_file = f\"{song_id}_{chunk_id}.wav\"\n",
    "\n",
    "      # Save the segment\n",
    "      torchaudio.save(\n",
    "        os.path.join(SEGMENTS_DIR, segment_file),\n",
    "        segment.cpu().squeeze(0),\n",
    "        sr,\n",
    "      )\n",
    "\n",
    "      records.append(\n",
    "        save_spectogram(mel_spec, song_id, chunk_id, \"original\")\n",
    "      )\n",
    "\n",
    "      chunk_id += 1\n",
    "    \n",
    "  # Save the dataframe\n",
    "  df = pd.DataFrame(records)\n",
    "  df.to_csv(DATASET_FILE, index=False)\n",
    "\n",
    "if (len(os.listdir(SPECTOGRAMS_DIR)) == 0 or len(os.listdir(SEGMENTS_DIR)) == 0):\n",
    "  print(\"Not founded spectograms, extracting them...\")\n",
    "  extract_features()\n",
    "else:\n",
    "  print(\"Founded spectograms, skiping it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd9c4d",
   "metadata": {},
   "source": [
    "# Balance classes using augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b2ba01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset already has augmented data, skipping augmentation.\n"
     ]
    }
   ],
   "source": [
    "augmentations = {\n",
    "    \"background_noise\": AddBackgroundNoise(\n",
    "      p=0.5,\n",
    "      background_paths=BACKGROUND_NOISE_DIR,\n",
    "      min_snr_in_db=10,\n",
    "      max_snr_in_db=20,\n",
    "      output_type=\"dict\"\n",
    "    ),  \n",
    "    \"colored_noise\": AddColoredNoise(\n",
    "      p=0.5,\n",
    "      min_snr_in_db=10,\n",
    "      max_snr_in_db=20,\n",
    "      output_type=\"dict\"\n",
    "    ),\n",
    "    \"gain\": Gain(\n",
    "      p=0.5,  \n",
    "      min_gain_in_db=-10,\n",
    "      max_gain_in_db=20,\n",
    "      output_type=\"dict\"\n",
    "    ),\n",
    "    \"pitch_shift\": PitchShift(\n",
    "      p=0.5,\n",
    "      min_transpose_semitones=-2,\n",
    "      max_transpose_semitones=2,\n",
    "      sample_rate=TARGET_SAMPLE_RATE,\n",
    "      output_type=\"dict\"\n",
    "    ),\n",
    "  }\n",
    "\n",
    "def augment_data():\n",
    "  df = pd.read_csv(DATASET_FILE)\n",
    "\n",
    "  if (df[\"aug_name\"] == \"original\").all():\n",
    "    print(\"The dataset only has original data, applying augmentation.\")\n",
    "\n",
    "    # Get the number of clases per song_id\n",
    "    print(df[\"song_id\"].value_counts())\n",
    "\n",
    "    number_of_samples_target = df[\"song_id\"].value_counts().max() * 1.5\n",
    "\n",
    "    print(f\"Target number of samples: {number_of_samples_target}\")\n",
    "\n",
    "    unique_song_ids = df[\"song_id\"].unique()\n",
    "    \n",
    "    for song_id in tqdm(unique_song_ids, desc=\"Augmenting data\", unit=\"song\"):\n",
    "      song_df = df[df[\"song_id\"] == song_id]\n",
    "\n",
    "      while df[df[\"song_id\"] == song_id].shape[0] < number_of_samples_target:\n",
    "        # Get a random sample of the original data\n",
    "        sample = song_df.sample(n=1, random_state=np.random.randint(0, 10000)).iloc[0]\n",
    "        segment_file = f\"{song_id}_{sample['chunk_id']}.wav\"\n",
    "        file_path = os.path.join(SEGMENTS_DIR, segment_file)\n",
    "\n",
    "        # Apply a random augmentation\n",
    "        aug_name = np.random.choice(list(augmentations.keys()))\n",
    "        augment = augmentations[aug_name]\n",
    "\n",
    "        waveform, sr = get_waveform_n_sr_from_file(file_path)\n",
    "        mel_spec = get_spectogram(waveform)\n",
    "        augmented_segment = augment(waveform, sample_rate=TARGET_SAMPLE_RATE)['samples']\n",
    "        mel_spec = get_spectogram(augmented_segment)\n",
    "        mel_spec = (mel_spec - global_mean) / global_std  \n",
    "\n",
    "        # Check if its a variation\n",
    "        duplicated = df[\n",
    "          (df[\"song_id\"] == song_id) & \n",
    "          (df[\"chunk_id\"] == sample[\"chunk_id\"]) &\n",
    "          (df[\"aug_name\"] == aug_name)\n",
    "        ]\n",
    "\n",
    "        if duplicated.shape[0] > 0:\n",
    "          variation_num = duplicated[\"variation\"].max() + 1\n",
    "        else:\n",
    "          variation_num = 0\n",
    "        \n",
    "\n",
    "        record = save_spectogram(\n",
    "          mel_spec,\n",
    "          song_id,\n",
    "          sample[\"chunk_id\"],\n",
    "          aug_name,\n",
    "          variation=variation_num \n",
    "        )\n",
    "\n",
    "        # Append the record to the dataframe\n",
    "        df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)\n",
    "\n",
    "    df.to_csv(DATASET_FILE, index=False)\n",
    "    print(df[\"song_id\"].value_counts())\n",
    "\n",
    "  else:\n",
    "    print(\"The dataset already has augmented data, skipping augmentation.\")\n",
    "    return\n",
    "  \n",
    "augment_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9b536",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0908cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectogramDataset(Dataset):\n",
    "  def __init__(self, spectograms_dir=SPECTOGRAMS_DIR, transform=None):\n",
    "    self.spectograms_files = os.listdir(SPECTOGRAMS_DIR)\n",
    "    self.spectograms_dir = spectograms_dir\n",
    "    self.transform = transform\n",
    "    dataset = pd.read_csv(DATASET_FILE)\n",
    "    self.labels = dataset[\"song_id\"]\n",
    "    self.classes = dataset['song_id'].unique()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.spectograms_files)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    file_name = self.spectograms_files[idx]\n",
    "    file_path = os.path.join(self.spectograms_dir, file_name)\n",
    "\n",
    "    mel_spec = np.load(file_path)[\"data\"] \n",
    "\n",
    "    if mel_spec.ndim == 4:\n",
    "      mel_spec = mel_spec.squeeze(0).squeeze(0)\n",
    "    elif mel_spec.ndim == 3:\n",
    "      mel_spec = mel_spec.squeeze(0) \n",
    "        \n",
    "    mel_spec = mel_spec[np.newaxis, ...]\n",
    "\n",
    "    label = int(file_name.split(\"_\")[0])  # Get the song id from the file name \n",
    "\n",
    "    if self.transform:\n",
    "      mel_spec = self.transform(mel_spec) \n",
    "\n",
    "    mel_spec = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "    label = torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    return mel_spec, label\n",
    "\n",
    "def get_spectograms():\n",
    "  dataset = SpectogramDataset()\n",
    "\n",
    "  idx = list(range(len(dataset)))\n",
    "  train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=dataset.labels)\n",
    "\n",
    "  train_subset = Subset(dataset, train_idx)   \n",
    "  test_subset  = Subset(dataset, test_idx)\n",
    "\n",
    "  return train_subset, test_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff25f6",
   "metadata": {},
   "source": [
    "# Using a CNN te extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Model not trained. Training model with cuda...\n",
      "Epoch 0: Train Loss 1.4961, Val Loss 0.6684, Train Acc 0.5241, Val Acc 0.7685, ETA: 182.68736600875854\n",
      "Epoch 1: Train Loss 0.4300, Val Loss 0.3769, Train Acc 0.8670, Val Acc 0.8826, ETA: 181.0255743265152\n",
      "Epoch 2: Train Loss 0.2184, Val Loss 0.1053, Train Acc 0.9321, Val Acc 0.9775, ETA: 179.66300201416016\n",
      "Epoch 3: Train Loss 0.0800, Val Loss 0.0925, Train Acc 0.9767, Val Acc 0.9743, ETA: 176.1690558195114\n",
      "Epoch 4: Train Loss 0.0499, Val Loss 0.2922, Train Acc 0.9863, Val Acc 0.9035, ETA: 172.19336433410646\n",
      "Epoch 5: Train Loss 0.0726, Val Loss 0.0750, Train Acc 0.9767, Val Acc 0.9678, ETA: 167.68301367759705\n",
      "Epoch 6: Train Loss 0.0301, Val Loss 0.0313, Train Acc 0.9920, Val Acc 0.9904, ETA: 163.3825080054147\n",
      "Epoch 7: Train Loss 0.0130, Val Loss 0.0236, Train Acc 0.9984, Val Acc 0.9936, ETA: 159.509246468544\n",
      "Epoch 8: Train Loss 0.0116, Val Loss 0.0679, Train Acc 0.9984, Val Acc 0.9871, ETA: 155.5552970568339\n",
      "Epoch 9: Train Loss 0.0242, Val Loss 0.0580, Train Acc 0.9940, Val Acc 0.9823, ETA: 151.76247990131378\n",
      "Epoch 10: Train Loss 0.0140, Val Loss 0.1018, Train Acc 0.9984, Val Acc 0.9743, ETA: 148.68167010220614\n",
      "Epoch 11: Train Loss 0.1125, Val Loss 0.1502, Train Acc 0.9666, Val Acc 0.9598, ETA: 145.21603327989578\n",
      "Epoch 12: Train Loss 0.0600, Val Loss 0.0714, Train Acc 0.9811, Val Acc 0.9775, ETA: 141.48915353188147\n",
      "Epoch 13: Train Loss 0.0194, Val Loss 0.2203, Train Acc 0.9952, Val Acc 0.9341, ETA: 137.7831826210022\n",
      "Early stopping at epoch 14/50\n",
      "Best model loaded from epoch 14/50\n",
      "Model saved to ./cache/trained_model.pt\n",
      "Model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, num_classes=256):\n",
    "    super(CNN, self).__init__()\n",
    "\n",
    "    self.conv_block = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout2d(0.1),\n",
    "      nn.MaxPool2d((2, 1)),\n",
    "      \n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d((2, 2)),\n",
    "\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d((2, 2)),\n",
    "\n",
    "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(256),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.AdaptiveAvgPool2d((4, 4)),\n",
    "    )\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(256 * 4 * 4, 1024),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.3),\n",
    "      nn.Linear(1024, num_classes),\n",
    "    )\n",
    "\n",
    "    self.scaler = GradScaler(device)\n",
    "\n",
    "    self.training_ETA = 0.0\n",
    "    self.total_epochs = 0\n",
    "    self.current_epoch = 0\n",
    "    self.is_model_trained = False\n",
    "  \n",
    "  def forward(self, x):\n",
    "    if x.dim() > 4:\n",
    "      x = x.squeeze(1)\n",
    "\n",
    "    x = self.conv_block(x)\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "  def fit(self, train_loader, test_loader, epochs=50, learning_rate=0.001, patience=7):\n",
    "    self.total_epochs = epochs\n",
    "    training_epoch_duration = []\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      self.train()\n",
    "      epoch_loss = 0.0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      start_time = time.time()\n",
    "      self.current_epoch = epoch\n",
    "\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type=device):\n",
    "          outputs = self(inputs)\n",
    "          loss = loss_function(outputs, labels)\n",
    "        \n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "      \n",
    "      # Calculate the average loss for the epoch\n",
    "      train_acc = correct / total\n",
    "      avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "      # Validation\n",
    "      val_loss = 0.0\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      self.eval()\n",
    "      with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "          inputs = inputs.to(device, non_blocking=True)\n",
    "          labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "          with autocast(device_type=device):\n",
    "            outputs = self(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "          val_loss += loss.item()\n",
    "          _, predicted = outputs.max(1)\n",
    "          total += labels.size(0)\n",
    "          correct += predicted.eq(labels).sum().item()\n",
    "      \n",
    "      val_acc = correct / total\n",
    "      avg_val_loss = val_loss / len(test_loader)\n",
    "      scheduler.step(avg_val_loss)\n",
    "\n",
    "      # Early stopping\n",
    "      if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = self.state_dict()    \n",
    "        patience_counter = 0\n",
    "      else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "          print(f\"Early stopping at epoch {epoch}/{epochs}\")\n",
    "          break\n",
    "\n",
    "      # Calculate the training ETA\n",
    "      elapsed_time = time.time() - start_time\n",
    "      training_epoch_duration.append(elapsed_time)\n",
    "      self.training_ETA = (sum(training_epoch_duration) / (self.current_epoch + 1)) * (self.total_epochs - self.current_epoch)\n",
    "\n",
    "      print(f\"Epoch {epoch}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}, ETA: {self.training_ETA}\")\n",
    "\n",
    "    if best_model_state:\n",
    "      self.load_state_dict(best_model_state)\n",
    "      print(f\"Best model loaded from epoch {self.current_epoch}/{self.total_epochs}\")\n",
    "    self.is_model_trained = True\n",
    "\n",
    "  def predict(self, X):\n",
    "    self.eval()\n",
    "\n",
    "    X = torch.as_tensor(X, dtype=torch.float32)\n",
    "\n",
    "    if isinstance(X, list):\n",
    "        X = torch.stack(X)\n",
    "    elif X.ndim == 2:\n",
    "        X = X.unsqueeze(0)\n",
    "\n",
    "    X = X.to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(device_type=device):\n",
    "        embeddings = self(X)\n",
    "\n",
    "    embeddings = embeddings.cpu()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "  def save_trained_model(self):\n",
    "    # Save the model\n",
    "    torch.save(self.state_dict(), TRAINED_MODEL_FILE)\n",
    "\n",
    "    print(f\"Model saved to {TRAINED_MODEL_FILE}\")\n",
    "  \n",
    "  def load_trained_model(self):\n",
    "    # Load the model\n",
    "    map_location = torch.device(device)\n",
    "    self.load_state_dict(torch.load(TRAINED_MODEL_FILE, map_location=map_location))\n",
    "    self.eval()\n",
    "    self.is_model_trained = True\n",
    "\n",
    "def initialize_model():\n",
    "  train_ds, test_ds = get_spectograms()\n",
    "\n",
    "  # Create DataLoaders for train and test sets\n",
    "  train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "  test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "  n_classes = len(list(set(train_loader.dataset.dataset.classes) | set(test_loader.dataset.dataset.classes)))\n",
    "  print(f\"Num of classes: {n_classes}\")\n",
    "\n",
    "  model = CNN(n_classes).to(device)\n",
    "\n",
    "  if torch.__version__ >= '2.0':\n",
    "    model = torch.compile(model)\n",
    "\n",
    "  train_model(model, train_loader, test_loader)\n",
    "\n",
    "  return model \n",
    "\n",
    "def train_model(model, train_loader, test_loader):\n",
    "  # Check if the model is already trained\n",
    "  if (os.path.exists(TRAINED_MODEL_FILE)):\n",
    "    print(f\"Model already trained. Loading model...\")\n",
    "    model.load_trained_model()\n",
    "  else:\n",
    "    print(f\"Model not trained. Training model with {device}...\")\n",
    "    model.fit(train_loader=train_loader, test_loader=test_loader, epochs=50, learning_rate=0.001)\n",
    "    model.save_trained_model()\n",
    "    print(f\"Model trained and saved.\")\n",
    "\n",
    "model = initialize_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
